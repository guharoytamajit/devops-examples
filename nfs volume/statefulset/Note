>mkdir -p /mnt/sharedfolder/{pv0,pv1,pv2,pv3,pv4}
>kubectl apply -f sts-pv.yaml
>kubectl apply -f sts-nginx.yaml


>kubectl get pv,pvc          #we can see only two pv are bound as replicas=2,other pv are available for use                    
NAME                          CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                  STORAGECLASS   REASON   AGE
persistentvolume/pv-nfs-pv0   200Mi      RWO            Retain           Bound       demo/www-nginx-sts-0   manual                  12m
persistentvolume/pv-nfs-pv1   200Mi      RWO            Retain           Bound       demo/www-nginx-sts-1   manual                  12m
persistentvolume/pv-nfs-pv2   200Mi      RWO            Retain           Available                          manual                  12m
persistentvolume/pv-nfs-pv3   200Mi      RWO            Retain           Available                          manual                  12m
persistentvolume/pv-nfs-pv4   200Mi      RWO            Retain           Available                          manual                  12m

NAME                                    STATUS   VOLUME       CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/www-nginx-sts-0   Bound    pv-nfs-pv0   200Mi      RWO            manual         12m
persistentvolumeclaim/www-nginx-sts-1   Bound    pv-nfs-pv1   200Mi      RWO            manual         98s

>kubectl get statefulset,po,svc,endpoints -o wide
NAME                         READY   AGE   CONTAINERS   IMAGES
statefulset.apps/nginx-sts   2/2     17m   nginx        nginx

NAME              READY   STATUS    RESTARTS   AGE   LABELS
pod/nginx-sts-0   1/1     Running   0          58m   controller-revision-hash=nginx-sts-748657747c,run=nginx-sts-demo,statefulset.kubernetes.io/pod-name=nginx-sts-0
pod/nginx-sts-1   1/1     Running   0          48m   controller-revision-hash=nginx-sts-748657747c,run=nginx-sts-demo,statefulset.kubernetes.io/pod-name=nginx-sts-1



NAME                     TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE   SELECTOR
service/nginx-headless   ClusterIP   None         <none>        80/TCP    17m   run=nginx-sts-demo

NAME                       ENDPOINTS                       AGE
endpoints/nginx-headless   10.244.1.64:80,10.244.2.66:80   17m


where dns => ip is:
nginx-sts-0.nginx-headless.demo.svc.cluster.local =>10.244.2.66
nginx-sts-1.nginx-headless.demo.svc.cluster.local =>10.244.1.64
<StatefulSet(nginx-sts)>-<Ordinal>.<Service(nginx-headless)>.<Namespace(demo)>.svc.cluster.local


Here ping to headless service are not load balanced but has the following behaviour:
From nginx-sts-0 pod
>ping nginx-headless   #pings nginx-sts-0.nginx-headless.demo.svc.cluster.local

From nginx-sts-1 pod
>ping nginx-headless   #pings nginx-sts-1.nginx-headless.demo.svc.cluster.local

From another pod outside statefulset,it will load balance
>ping nginx-headless  #sometimes ping nginx-sts-0.nginx-headless.demo.svc.cluster.local,sometime pings nginx-sts-0.nginx-headless.demo.svc.cluster.local

----------------------------------------
If headless service does not load balancee them from outside cluster?
How do you properly expose a StatefulSet externally?
Ans:  The created headless service will not be given a clusterIP, but will instead simply include a list of Endpoints. These Endpoints are then used to generate instance-specific DNS records in the form of: <StatefulSet>-<Ordinal>.<Service>.<Namespace>.svc.cluster.local e.g., app-0.myapp.default.svc.cluster.local.

Then how to loadbalance across stateful pods for non-cluster requests?
Ans: The Kubernetes developers have thought of this already. ;) Each Pod in the StatefulSet has itâ€™s own label that includes its generated Pod identity: statefulset.kubernetes.io/pod-name. You know what that means? It means we can use it in a Service Selector!
While the headless service might not take care of all our needs, we can create additional Services that point to the individual Pods of the StatefulSet. 

apiVersion: v1
kind: Service
metadata:
  name: my-app
spec:
  type: LoadBalancer
  selector:
    run: nginx-sts-demo
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
==============
cleanup:
kill all pods,svc,statefulset:
>kubectl scale sts nginx-sts --replicas=0
>kubectl delete all --all
kill all pvc:
>kubectl delete pvc --all

used pv will now be in released state but still not available for use:
>kubectl get pv    
NAME         CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                  STORAGECLASS   REASON   AGE
pv-nfs-pv0   200Mi      RWO            Retain           Released    demo/www-nginx-sts-0   manual                  82m
pv-nfs-pv1   200Mi      RWO            Retain           Released    demo/www-nginx-sts-1   manual                  82m
pv-nfs-pv2   200Mi      RWO            Retain           Available                          manual                  82m
pv-nfs-pv3   200Mi      RWO            Retain           Available                          manual                  82m
pv-nfs-pv4   200Mi      RWO            Retain           Available                          manual                  82m


so we have to delete pv also:
>kubectl delete pv --all






