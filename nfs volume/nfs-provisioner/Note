Why dynamic provisioner?
cluster admin has to manually defune and update all pv.
this pv is consumed by pvc+deployment/pod   or by statefulset with volumeClaimTemplates.
this will automate the creation of pv 
Now user can request pv using pvc or volumeClaimTemplates, behind the scene there is a provisioner(pod) that creates pv.
If user deletes pvc the associated pv will also be deleted by this provisioner(pod)


nfs-client-provisioner:
The NFS client provisioner is an automatic provisioner for Kubernetes that uses your already configured NFS server, automatically creating Persistent Volumes.
https://github.com/helm/charts/tree/master/stable/nfs-client-provisioner

ks8 components to create:

1)RBAC yaml: Create all role bindings for our service account.this service account will be used by provisioner pod defined in step 3.
>kubectl create -f rbac.yaml
-----------------------------
kind: ServiceAccount
apiVersion: v1
metadata:
  name: nfs-client-provisioner
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-client-provisioner-runner
rules:
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "update", "patch"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-client-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    namespace: default
roleRef:
  kind: ClusterRole
  name: nfs-client-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: default
roleRef:
  kind: Role
  name: leader-locking-nfs-client-provisioner
  apiGroup: rbac.authorization.k8s.io
-------------------------------

2)StarageClass
>kubectl create -f class.yaml
--------------------------
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: managed-nfs-storage                  #we have to refer this storage class when we are defining pvc,to use this provisioner
provisioner: example.com/nfs
parameters:
  archiveOnDelete: "false"


-------------------------
3)Privisioner Deployment with one pod: This pod created from nfs-client-provisioner:latest will create pv
>kubectl create -f provisioner-deployment.yaml
--------------------------
kind: Deployment
apiVersion: apps/v1
metadata:
  name: nfs-client-provisioner
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: nfs-client-provisioner
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: quay.io/external_storage/nfs-client-provisioner:latest
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: example.com/nfs
            - name: NFS_SERVER
              value: 192.168.0.105
            - name: NFS_PATH
              value: /mnt/sharedfolder
      volumes:
        - name: nfs-client-root
          nfs:
            server: 192.168.0.105
            path: /mnt/sharedfolder


--------------------------
>kubectl describe <nfs-client-provisioner pod>
here you will see the nfs has been mounted on /persistentvolume path.

Now id=f create any file inside /persistentvolume of <nfs-client-provisioner pod>,we can see the same file in /mnt/sharedfolder

>kubectl get pv,pvc
>ls /mnt/sharedfolder
Still no pv or pvc is created.

Note:this pod is responsible for creating a new pv when a new pvc is created 

-------------------------------------
Now when we deloy deployment or statefulset with pvc or volumeClaimTemplates,then pv and pvc will be created in pair.

>kubectl create -f pvc-nfs.yaml
-------------------
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc1
spec:
  storageClassName: managed-nfs-storage
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 500Mi
--------------------

>kubectl get pv,pvc
>ls /mnt/sharedfolder
pv will be created for the created pvc

Now lets create a pod and put some file to see if it is using pv:
>kubectl create -f busybox.yaml
----------------------------
apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  volumes:
  - name: host-volume
    persistentVolumeClaim:
      claimName: pvc1
  containers:
  - image: busybox
    name: busybox
    command: ["/bin/sh"]
    args: ["-c", "sleep 600"]
    volumeMounts:
    - name: host-volume
      mountPath: /mydata
------------------------

>kubectl exec busybox -it sh
$echo hello>/mydata/helloworld
$exit

now check NSF directory if it contails hellowerld file.
>tree /mnt/sharedfolder






